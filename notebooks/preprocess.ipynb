{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 19:38:43 WARN Utils: Your hostname, DESKTOP-AKL6QQR resolves to a loopback address: 127.0.1.1; using 172.24.95.98 instead (on interface eth0)\n",
      "22/08/21 19:38:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 19:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Project 1 Preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", True)\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"3g\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Parquets for Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf1 = spark.read.parquet('../data/raw/2018-01.parquet')\n",
    "sdf2 = spark.read.parquet('../data/raw/2018-02.parquet')\n",
    "sdf3 = spark.read.parquet('../data/raw/2018-03.parquet')\n",
    "sdf4 = spark.read.parquet('../data/raw/2018-04.parquet')\n",
    "sdf5 = spark.read.parquet('../data/raw/2018-05.parquet')\n",
    "sdf6 = spark.read.parquet('../data/raw/2018-06.parquet')\n",
    "sdf7 = spark.read.parquet('../data/raw/2018-07.parquet')\n",
    "sdf8 = spark.read.parquet('../data/raw/2018-08.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Month Column to Each Dataframe - Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "sdf1 = sdf1.withColumn('Month', lit(\"Jan\"))\n",
    "sdf2 = sdf2.withColumn('Month', lit(\"Feb\"))\n",
    "sdf3 = sdf3.withColumn('Month', lit(\"Mar\"))\n",
    "sdf4 = sdf4.withColumn('Month', lit(\"Apr\"))\n",
    "sdf5 = sdf5.withColumn('Month', lit(\"May\"))\n",
    "sdf6 = sdf6.withColumn('Month', lit(\"Jun\"))\n",
    "sdf7 = sdf7.withColumn('Month', lit(\"Jul\"))\n",
    "sdf8 = sdf8.withColumn('Month', lit(\"Aug\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate Dataframes - Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>Month</th></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:21:05</td><td>2018-01-01 00:24:23</td><td>1.0</td><td>0.5</td><td>1.0</td><td>N</td><td>41</td><td>24</td><td>2</td><td>4.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>5.8</td><td>null</td><td>null</td><td>Jan</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:44:55</td><td>2018-01-01 01:03:05</td><td>1.0</td><td>2.7</td><td>1.0</td><td>N</td><td>239</td><td>140</td><td>2</td><td>14.0</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>15.3</td><td>null</td><td>null</td><td>Jan</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:08:26</td><td>2018-01-01 00:14:21</td><td>2.0</td><td>0.8</td><td>1.0</td><td>N</td><td>262</td><td>141</td><td>1</td><td>6.0</td><td>0.5</td><td>0.5</td><td>1.0</td><td>0.0</td><td>0.3</td><td>8.3</td><td>null</td><td>null</td><td>Jan</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:20:22</td><td>2018-01-01 00:52:51</td><td>1.0</td><td>10.2</td><td>1.0</td><td>N</td><td>140</td><td>257</td><td>2</td><td>33.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.3</td><td>34.8</td><td>null</td><td>null</td><td>Jan</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:09:18</td><td>2018-01-01 00:27:06</td><td>2.0</td><td>2.5</td><td>1.0</td><td>N</td><td>246</td><td>239</td><td>1</td><td>12.5</td><td>0.5</td><td>0.5</td><td>2.75</td><td>0.0</td><td>0.3</td><td>16.55</td><td>null</td><td>null</td><td>Jan</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|Month|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----+\n",
       "|       1| 2018-01-01 00:21:05|  2018-01-01 00:24:23|            1.0|          0.5|       1.0|                 N|          41|          24|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|       null|  Jan|\n",
       "|       1| 2018-01-01 00:44:55|  2018-01-01 01:03:05|            1.0|          2.7|       1.0|                 N|         239|         140|           2|       14.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        15.3|                null|       null|  Jan|\n",
       "|       1| 2018-01-01 00:08:26|  2018-01-01 00:14:21|            2.0|          0.8|       1.0|                 N|         262|         141|           1|        6.0|  0.5|    0.5|       1.0|         0.0|                  0.3|         8.3|                null|       null|  Jan|\n",
       "|       1| 2018-01-01 00:20:22|  2018-01-01 00:52:51|            1.0|         10.2|       1.0|                 N|         140|         257|           2|       33.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        34.8|                null|       null|  Jan|\n",
       "|       1| 2018-01-01 00:09:18|  2018-01-01 00:27:06|            2.0|          2.5|       1.0|                 N|         246|         239|           1|       12.5|  0.5|    0.5|      2.75|         0.0|                  0.3|       16.55|                null|       null|  Jan|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataframe which is the concatenation of the 6 dataframes\n",
    "taxi_sdf = sdf1.union(sdf2)\n",
    "taxi_sdf = taxi_sdf.union(sdf3)\n",
    "taxi_sdf = taxi_sdf.union(sdf4)\n",
    "taxi_sdf = taxi_sdf.union(sdf5)\n",
    "taxi_sdf = taxi_sdf.union(sdf6)\n",
    "taxi_sdf = taxi_sdf.union(sdf7)\n",
    "taxi_sdf = taxi_sdf.union(sdf8)\n",
    "\n",
    "# number of combined rows for all dataframes\n",
    "totaln = sdf1.count() + sdf2.count() + sdf3.count() + sdf4.count() + sdf5.count() + sdf6.count() + sdf7.count() + sdf8.count()\n",
    "\n",
    "# check that the concatenation resulted in the correct number of rows\n",
    "print(bool(totaln == taxi_sdf.count()))\n",
    "\n",
    "taxi_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69636649, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# shape of dataset\n",
    "print((taxi_sdf.count(), len(taxi_sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the External Weather Data & Removing Irrelevant Columns - Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>WND</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TMP</th>\n",
       "      <th>AA1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>2018-08-31T20:51:00</td>\n",
       "      <td>100,5,N,0082,5</td>\n",
       "      <td>016093,5,N,5</td>\n",
       "      <td>+0228,5</td>\n",
       "      <td>01,0000,2,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9081</th>\n",
       "      <td>2018-08-31T21:00:00</td>\n",
       "      <td>100,1,N,0082,1</td>\n",
       "      <td>016000,1,9,9</td>\n",
       "      <td>+0228,1</td>\n",
       "      <td>03,0000,2,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>2018-08-31T21:51:00</td>\n",
       "      <td>110,5,N,0067,5</td>\n",
       "      <td>016093,5,N,5</td>\n",
       "      <td>+0228,5</td>\n",
       "      <td>01,0000,9,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>2018-08-31T22:51:00</td>\n",
       "      <td>090,5,N,0072,5</td>\n",
       "      <td>016093,5,N,5</td>\n",
       "      <td>+0222,5</td>\n",
       "      <td>01,0000,9,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>2018-08-31T23:51:00</td>\n",
       "      <td>100,5,N,0062,5</td>\n",
       "      <td>016093,5,N,5</td>\n",
       "      <td>+0217,5</td>\n",
       "      <td>01,0000,9,5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DATE             WND           VIS      TMP          AA1\n",
       "9080  2018-08-31T20:51:00  100,5,N,0082,5  016093,5,N,5  +0228,5  01,0000,2,5\n",
       "9081  2018-08-31T21:00:00  100,1,N,0082,1  016000,1,9,9  +0228,1  03,0000,2,1\n",
       "9082  2018-08-31T21:51:00  110,5,N,0067,5  016093,5,N,5  +0228,5  01,0000,9,5\n",
       "9083  2018-08-31T22:51:00  090,5,N,0072,5  016093,5,N,5  +0222,5  01,0000,9,5\n",
       "9084  2018-08-31T23:51:00  100,5,N,0062,5  016093,5,N,5  +0217,5  01,0000,9,5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weather_df = pd.read_csv('../data/raw/74486094789.csv', low_memory=False)\n",
    "\n",
    "# these are the same for every instance\n",
    "weather_df = weather_df.drop('STATION', axis=1)\n",
    "weather_df = weather_df.drop('NAME', axis=1)\n",
    "weather_df = weather_df.drop('ELEVATION', axis=1)\n",
    "weather_df = weather_df.drop('LATITUDE', axis=1)\n",
    "weather_df = weather_df.drop('LONGITUDE', axis=1)\n",
    "\n",
    "# drop every irrelevant column (keep only the \"Mandatory Data Section\")\n",
    "weather_df = weather_df.filter(['DATE','WND','VIS','TMP','AA1'])\n",
    "\n",
    "# drop instances after June 30th\n",
    "n = weather_df.index[weather_df['DATE'] == '2018-08-31T23:51:00'].tolist()\n",
    "weather_df = weather_df.iloc[:n[0]+1]\n",
    "\n",
    "weather_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperating the Combination Columns into Individual Columns - Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Visibility_Distance</th>\n",
       "      <th>Air_Temp</th>\n",
       "      <th>Precipitation_Depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>0093</td>\n",
       "      <td>016000</td>\n",
       "      <td>-0111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01T00:51:00</td>\n",
       "      <td>0067</td>\n",
       "      <td>016093</td>\n",
       "      <td>-0117</td>\n",
       "      <td>0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01T01:51:00</td>\n",
       "      <td>0093</td>\n",
       "      <td>016093</td>\n",
       "      <td>-0117</td>\n",
       "      <td>0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01T02:51:00</td>\n",
       "      <td>0093</td>\n",
       "      <td>016093</td>\n",
       "      <td>-0122</td>\n",
       "      <td>0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01T03:00:00</td>\n",
       "      <td>0093</td>\n",
       "      <td>016000</td>\n",
       "      <td>-0122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE Wind_Speed Visibility_Distance Air_Temp  \\\n",
       "0  2018-01-01T00:00:00       0093              016000    -0111   \n",
       "1  2018-01-01T00:51:00       0067              016093    -0117   \n",
       "2  2018-01-01T01:51:00       0093              016093    -0117   \n",
       "3  2018-01-01T02:51:00       0093              016093    -0122   \n",
       "4  2018-01-01T03:00:00       0093              016000    -0122   \n",
       "\n",
       "  Precipitation_Depth  \n",
       "0                 NaN  \n",
       "1                0000  \n",
       "2                0000  \n",
       "3                0000  \n",
       "4                 NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# separate the WND column but only keep wind speed\n",
    "weather_df[['1', '2', '3', 'Wind_Speed', '5']] = weather_df['WND'].str.split(',', expand=True) # names of the attributes are irrelevant\n",
    "weather_df = weather_df.drop(['WND','1','2','3','5'], axis=1)  \n",
    "\n",
    "# separate the VIS column but only keep visibility distance\n",
    "weather_df[['Visibility_Distance', '1', '2', '3']] = weather_df['VIS'].str.split(',', expand=True)\n",
    "weather_df = weather_df.drop(['VIS','1','2','3'], axis=1)\n",
    "\n",
    "# separate the TMP column but only keep air temperature\n",
    "weather_df[['Air_Temp', '1']] = weather_df['TMP'].str.split(',', expand=True)\n",
    "weather_df = weather_df.drop(['TMP','1'], axis=1)\n",
    "\n",
    "# separate the AA1 column but only keep precipitation depth\n",
    "weather_df[['1', 'Precipitation_Depth', '3', '4']] = weather_df['AA1'].str.split(',', expand=True)\n",
    "weather_df = weather_df.drop(['AA1','1','3','4'], axis=1) \n",
    "\n",
    "# turn missing values (often denoted by 9999) to NaN\n",
    "weather_df['Wind_Speed'] = weather_df['Wind_Speed'].replace('9999', np.NaN)\n",
    "weather_df['Visibility_Distance'] = weather_df['Visibility_Distance'].replace('999999', np.NaN)\n",
    "weather_df['Air_Temp'] = weather_df['Air_Temp'].replace('+9999', np.NaN)\n",
    "weather_df['Precipitation_Depth'] = weather_df['Precipitation_Depth'].replace('9999', np.NaN)\n",
    "\n",
    "weather_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9085, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensions before NULL value removal\n",
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating the Data to fill NaN Values & Correcting dtypes - Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE                      0\n",
      "Wind_Speed              251\n",
      "Visibility_Distance     251\n",
      "Air_Temp                252\n",
      "Precipitation_Depth    2056\n",
      "dtype: int64\n",
      "DATE                    0\n",
      "Wind_Speed              0\n",
      "Visibility_Distance     0\n",
      "Air_Temp                0\n",
      "Precipitation_Depth    79\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(weather_df.isna().sum())\n",
    "\n",
    "# remove duplicate rows - the data contains duplicate values in the DATE column\n",
    "weather_df = weather_df.drop_duplicates(subset=['DATE'], keep='first')\n",
    "\n",
    "# set the correct dtypes for each column\n",
    "weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])\n",
    "weather_df['Wind_Speed'] = pd.to_numeric(weather_df['Wind_Speed'])\n",
    "weather_df['Visibility_Distance'] = pd.to_numeric(weather_df['Visibility_Distance'])\n",
    "weather_df['Air_Temp'] = pd.to_numeric(weather_df['Air_Temp'])\n",
    "weather_df['Precipitation_Depth'] = pd.to_numeric(weather_df['Precipitation_Depth'])\n",
    "\n",
    "# correct the scale of Air_Temp\n",
    "weather_df['Air_Temp'] = weather_df['Air_Temp'].div(10).round(4)\n",
    "\n",
    "# linear interpolation with a limit on the maximum number of consecutive NaNs to fill (2)\n",
    "weather_df[['Wind_Speed','Visibility_Distance','Air_Temp','Precipitation_Depth']] = weather_df[['Wind_Speed','Visibility_Distance',\\\n",
    "    'Air_Temp','Precipitation_Depth']].interpolate(limit=2, limit_direction='forward')\n",
    "print(weather_df.isna().sum())\n",
    "\n",
    "# drop the rows that still contain NaN values - too much risk in interpolating further\n",
    "weather_df = weather_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8993, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 02:41:27 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 287563 ms exceeds timeout 120000 ms\n",
      "22/08/22 02:41:28 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# dimension after NULL value removal\n",
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Instances that did not Pay with Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.payment_type==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Columns that are not Required for Analysis - Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==================================================>     (87 + 9) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48224456, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['Store_and_fwd_flag','VendorID', 'fare_amount', 'RatecodeID', \\\n",
    "    'airport_fee', 'congestion_surcharge', 'payment_type'] # Airport fee & congestion surchage would have been\n",
    "                                                           # useful but they are empty.\n",
    "taxi_sdf = taxi_sdf.drop(*columns_to_drop)\n",
    "\n",
    "print((taxi_sdf.count(), len(taxi_sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a New Column for Total Extra Fees Paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "fees_sdf = taxi_sdf['Extra'] + taxi_sdf['MTA_tax'] + taxi_sdf['Improvement_surcharge']\n",
    "\n",
    "taxi_sdf = taxi_sdf.withColumn('Extra_fees_sum', fees_sdf)\n",
    "columns_to_drop = ['Extra', 'MTA_tax', 'Improvement_surcharge']\n",
    "taxi_sdf = taxi_sdf.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection & Removal - Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop trips with location IDs less than 1 or more than 263\n",
    "taxi_sdf = taxi_sdf.filter((taxi_sdf.PULocationID>=1) & (taxi_sdf.PULocationID<=263))\n",
    "taxi_sdf = taxi_sdf.filter((taxi_sdf.DOLocationID>=1) & (taxi_sdf.DOLocationID<=263))\n",
    "\n",
    "# drop trips with zero passengers\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.passenger_count>0)\n",
    "\n",
    "# drop trips with zero trip distance\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.trip_distance>0)\n",
    "\n",
    "# drop trips with negative duration\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.tpep_dropoff_datetime>taxi_sdf.tpep_pickup_datetime)\n",
    "\n",
    "# drop trips with zero total amount paid\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.total_amount>0)\n",
    "\n",
    "# drop trips that are out of the selected date range\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.tpep_dropoff_datetime>='2018-01-01 00:00:00').filter\\\n",
    "    (taxi_sdf.tpep_dropoff_datetime<='2018-08-31 23:59:59')\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf.tpep_pickup_datetime>='2018-01-01 00:00:00').filter\\\n",
    "    (taxi_sdf.tpep_pickup_datetime<='2018-08-31 23:59:59')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=====================================================>  (92 + 4) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46884259, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((taxi_sdf.count(), len(taxi_sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Taxi & Weather Dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>PULocationID</th><th>DOLocationID</th><th>tip_amount</th><th>tolls_amount</th><th>total_amount</th><th>Month</th><th>Extra_fees_sum</th><th>Avg_Wind_Speed</th><th>Avg_Visibility_Distance</th><th>Avg_Air_Temp</th><th>Avg_Precipitation_Depth</th></tr>\n",
       "<tr><td>2018-01-01 00:08:26</td><td>2018-01-01 00:14:21</td><td>2.0</td><td>0.8</td><td>262</td><td>141</td><td>1.0</td><td>0.0</td><td>8.3</td><td>Jan</td><td>1.3</td><td>82.359375</td><td>16072.65625</td><td>-11.246875</td><td>0.0</td></tr>\n",
       "<tr><td>2018-01-01 00:09:18</td><td>2018-01-01 00:27:06</td><td>2.0</td><td>2.5</td><td>246</td><td>239</td><td>2.75</td><td>0.0</td><td>16.55</td><td>Jan</td><td>1.3</td><td>82.359375</td><td>16072.65625</td><td>-11.246875</td><td>0.0</td></tr>\n",
       "<tr><td>2018-01-01 00:38:08</td><td>2018-01-01 00:48:24</td><td>2.0</td><td>1.7</td><td>50</td><td>239</td><td>2.05</td><td>0.0</td><td>12.35</td><td>Jan</td><td>1.3</td><td>82.359375</td><td>16072.65625</td><td>-11.246875</td><td>0.0</td></tr>\n",
       "<tr><td>2018-01-01 00:49:29</td><td>2018-01-01 00:51:53</td><td>1.0</td><td>0.7</td><td>239</td><td>238</td><td>1.0</td><td>0.0</td><td>6.3</td><td>Jan</td><td>1.3</td><td>82.359375</td><td>16072.65625</td><td>-11.246875</td><td>0.0</td></tr>\n",
       "<tr><td>2018-01-01 00:56:38</td><td>2018-01-01 01:01:05</td><td>1.0</td><td>1.0</td><td>238</td><td>24</td><td>1.7</td><td>0.0</td><td>8.5</td><td>Jan</td><td>1.3</td><td>82.359375</td><td>16072.65625</td><td>-11.246875</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------------+-------------+------------+------------+----------+------------+------------+-----+--------------+--------------+-----------------------+------------+-----------------------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|tip_amount|tolls_amount|total_amount|Month|Extra_fees_sum|Avg_Wind_Speed|Avg_Visibility_Distance|Avg_Air_Temp|Avg_Precipitation_Depth|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+----------+------------+------------+-----+--------------+--------------+-----------------------+------------+-----------------------+\n",
       "| 2018-01-01 00:08:26|  2018-01-01 00:14:21|            2.0|          0.8|         262|         141|       1.0|         0.0|         8.3|  Jan|           1.3|     82.359375|            16072.65625|  -11.246875|                    0.0|\n",
       "| 2018-01-01 00:09:18|  2018-01-01 00:27:06|            2.0|          2.5|         246|         239|      2.75|         0.0|       16.55|  Jan|           1.3|     82.359375|            16072.65625|  -11.246875|                    0.0|\n",
       "| 2018-01-01 00:38:08|  2018-01-01 00:48:24|            2.0|          1.7|          50|         239|      2.05|         0.0|       12.35|  Jan|           1.3|     82.359375|            16072.65625|  -11.246875|                    0.0|\n",
       "| 2018-01-01 00:49:29|  2018-01-01 00:51:53|            1.0|          0.7|         239|         238|       1.0|         0.0|         6.3|  Jan|           1.3|     82.359375|            16072.65625|  -11.246875|                    0.0|\n",
       "| 2018-01-01 00:56:38|  2018-01-01 01:01:05|            1.0|          1.0|         238|          24|       1.7|         0.0|         8.5|  Jan|           1.3|     82.359375|            16072.65625|  -11.246875|                    0.0|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+----------+------------+------------+-----+--------------+--------------+-----------------------+------------+-----------------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# find mean weather conditions for each day\n",
    "weather_df['DATE']= pd.to_datetime(weather_df['DATE'])\n",
    "weather_df['DATE'] = weather_df['DATE'].dt.date\n",
    "weather_df = pd.DataFrame(weather_df.groupby('DATE', as_index = False).mean())\n",
    "\n",
    "# turn weather dataframe from pandas to pyspark & rename columns\n",
    "w_sdf = spark.createDataFrame(weather_df)\n",
    "w_sdf = w_sdf.withColumnRenamed('Wind_Speed', 'Avg_Wind_Speed')\n",
    "w_sdf = w_sdf.withColumnRenamed('Visibility_Distance', 'Avg_Visibility_Distance')\n",
    "w_sdf = w_sdf.withColumnRenamed('Air_Temp', 'Avg_Air_Temp')\n",
    "w_sdf = w_sdf.withColumnRenamed('Precipitation_Depth', 'Avg_Precipitation_Depth')\n",
    "\n",
    "# create a column with just the date (not time) to make the dataframe joins easier\n",
    "taxi_sdf = taxi_sdf.withColumn('DATE', F.to_date(F.col('tpep_pickup_datetime')))\n",
    "\n",
    "# join the two dataframes\n",
    "joint_sdf = taxi_sdf.join(w_sdf, taxi_sdf['DATE'] == w_sdf['DATE'], 'left')\n",
    "\n",
    "# drop the columns not required any further \n",
    "columns_to_drop = ['DATE', 'DATE']\n",
    "joint_sdf = joint_sdf.drop(*columns_to_drop)\n",
    "\n",
    "joint_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOLocationID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra_fees_sum</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Wind_Speed</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Visibility_Distance</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Air_Temp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Precipitation_Depth</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count\n",
       "passenger_count              0\n",
       "trip_distance                0\n",
       "PULocationID                 0\n",
       "DOLocationID                 0\n",
       "tip_amount                   0\n",
       "tolls_amount                 0\n",
       "total_amount                 0\n",
       "Extra_fees_sum               0\n",
       "Avg_Wind_Speed               0\n",
       "Avg_Visibility_Distance      0\n",
       "Avg_Air_Temp                 0\n",
       "Avg_Precipitation_Depth      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def count_missings(spark_df, sort=True): # Function from user \"gench\" - https://stackoverflow.com/questions/44627386/\n",
    "                                         # how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type)\\\n",
    "        in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "count_missings(joint_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Data to Curated Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joint_sdf.write.mode('overwrite').parquet('../data/curated/processed_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
